{
 "metadata": {
  "name": "NLTK_DEBUG"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "import nltk as nltk\n",
      "#nltk.download()\n",
      "from nltk.corpus import stopwords\n",
      "import os, os.path\n",
      "path = os.path.expanduser('~/nltk_data')\n",
      "if not os.path.exists(path):\n",
      "    os.mkdir(path)\n",
      "os.path.exists(path)\n",
      "import nltk.data\n",
      "path in nltk.data.path\n",
      "from nltk.corpus.reader import CategorizedPlaintextCorpusReader\n",
      "reader = CategorizedPlaintextCorpusReader('.', r'.*_news_.*\\.csv', cat_pattern=r'.*_news_(\\w+)\\.csv')\n",
      "reader.categories()\n",
      "reader.fileids(categories=['UP'])\n",
      "def bag_of_words(words):\n",
      "    return dict([(word, True) for word in words if word[0].isalpha()])\n",
      "import collections\n",
      "def bag_of_words_not_in_set(words, badwords):\n",
      "    return bag_of_words(set(words)-set(badwords))\n",
      "\n",
      "def bag_of_non_stopwords(words, stopfile='english'):\n",
      "    badwords = stopwords.words(stopfile)\n",
      "    return bag_of_words_not_in_set(words, badwords)\n",
      "\n",
      "from nltk.metrics import BigramAssocMeasures\n",
      "from nltk.collocations import BigramCollocationFinder\n",
      "\n",
      "def bag_of_bigrams_words(words, score_fn=BigramAssocMeasures.chi_sq, n=200):\n",
      "    bigram_finder = BigramCollocationFinder.from_words(words)\n",
      "    bigrams = bigram_finder.nbest(score_fn, n)\n",
      "    return bag_of_words(words + bigrams.words())\n",
      "    \n",
      "def label_feats_from_corpus(corp, feature_detector=bag_of_bigrams_words):\n",
      "    label_feats = collections.defaultdict(list)\n",
      "    for label in corp.categories():\n",
      "        for fileid in corp.fileids(categories=[label]):\n",
      "            feats = feature_detector(corp.words(fileids=[fileid]))\n",
      "            label_feats[label].append(feats)\n",
      "    return label_feats\n",
      "\n",
      "def split_label_feats(lfeats, split=0.90):\n",
      "    train_feats = []\n",
      "    test_feats = []\n",
      "    for label, feats in lfeats.iteritems():\n",
      "        cutoff = int(len(feats) * split)\n",
      "        train_feats.extend([(feat, label) for feat in feats[:cutoff]])\n",
      "        test_feats.extend([(feat, label) for feat in feats[cutoff:]])\n",
      "    return train_feats, test_feats\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reader.categories()\n",
      "\n",
      "lfeats = label_feats_from_corpus(reader)\n",
      "lfeats.keys()\n",
      "train_feats, test_feats = split_label_feats(lfeats)\n",
      "len(train_feats)\n",
      "len(test_feats)\n",
      "\n",
      "from nltk.classify import NaiveBayesClassifier\n",
      "nb_classifier = NaiveBayesClassifier.train(train_feats)\n",
      "nb_classifier.labels()\n",
      "\n",
      "from nltk.classify.util import accuracy\n",
      "accuracy(nb_classifier, test_feats)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk as nltk\n",
      "#nltk.download()\n",
      "from nltk.corpus import stopwords\n",
      "import os, os.path\n",
      "path = os.path.expanduser('~/nltk_data')\n",
      "if not os.path.exists(path):\n",
      "    os.mkdir(path)\n",
      "os.path.exists(path)\n",
      "import nltk.data\n",
      "path in nltk.data.path\n",
      "from nltk.corpus.reader import CategorizedPlaintextCorpusReader\n",
      "reader = CategorizedPlaintextCorpusReader('.', r'.*_news_.*\\.csv', cat_pattern=r'.*_news_(\\w+)\\.csv')\n",
      "reader.categories()\n",
      "reader.fileids(categories=['UP'])\n",
      "def bag_of_words(words):\n",
      "    return dict([(word, True) for word in words if word[0].isalpha()])\n",
      "import collections\n",
      "def bag_of_words_not_in_set(words, badwords):\n",
      "    return bag_of_words(set(words)-set(badwords))\n",
      "\n",
      "def bag_of_non_stopwords(words, stopfile='english'):\n",
      "    badwords = stopwords.words(stopfile)\n",
      "    return bag_of_words_not_in_set(words, badwords)\n",
      "\n",
      "from nltk.metrics import BigramAssocMeasures\n",
      "from nltk.collocations import BigramCollocationFinder"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "label_feats = collections.defaultdict(list)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bigram_finder = BigramCollocationFinder.from_words(words)\n",
      "    bigrams = bigram_finder.nbest(score_fn, n)\n",
      "\n",
      "for label in reader.categories():\n",
      "        for fileid in reader.fileids(categories=[label]):\n",
      "            feats = feature_detector(reader.words(fileids=[fileid]))\n",
      "            label_feats[label].append(feats)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'feature_detector' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-4-7582710d8cc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfileid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m             \u001b[0mfeats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_detector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfileid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m             \u001b[0mlabel_feats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: name 'feature_detector' is not defined"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}